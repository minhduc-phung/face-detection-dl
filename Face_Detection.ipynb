{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "image_folder = \"./image_face_detection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms.functional as fn\n",
    "import torchvision.ops as ops\n",
    "from net import Net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detection_net = Net()\n",
    "face_detection_net.load_state_dict(torch.load(\"D:\\5IF\\OT2_ML\\deep_learning_project\\saved_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3]]\n"
     ]
    }
   ],
   "source": [
    "x = [[1,2,3],[2,3,4]]\n",
    "print(x[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variables used for the object detection procedure\n",
    "WIDTH = 600\n",
    "PYR_SCALE = 1.5\n",
    "WINDOW_STEP = 24\n",
    "ROI_SIZE = (168,168)\n",
    "INPUT_SIZE = (36,36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the region of interest parameters\n",
    "left = 0  # Define the left coordinate of the region\n",
    "top = 0  # Define the top coordinate of the region\n",
    "width = 36  # Define the width of the region\n",
    "height = 36  # Define the height of the region\n",
    "\n",
    "rois = []\n",
    "locs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(image, step, ws):\n",
    "\t# slide a window across the image\n",
    "\tfor y in range(0, image.shape[0] - ws[1], step):\n",
    "\t\tfor x in range(0, image.shape[1] - ws[0], step):\n",
    "\t\t\t# yield the current window\n",
    "\t\t\tyield (x, y, image[y:y + ws[1], x:x + ws[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_pyramid(image, scale=1.5, minSize=(168, 168)):\n",
    "\t# yield the original image\n",
    "\tyield image\n",
    "\t# keep looping over the image pyramid\n",
    "\twhile True:\n",
    "\t\t# compute the dimensions of the next image in the pyramid\n",
    "\t\tw = int(image.shape[1] / scale)\n",
    "\t\timage = fn.resize(image, w)\n",
    "\t\t# if the resized image does not meet the supplied minimum\n",
    "\t\t# size, then stop constructing the pyramid\n",
    "\t\tif image.shape[0] < minSize[1] or image.shape[1] < minSize[0]:\n",
    "\t\t\tbreak\n",
    "\t\t# yield the next image in the pyramid\n",
    "\t\tyield image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([  \n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((36,36)),\n",
    "    transforms.PILToTensor()\n",
    "])\n",
    "\n",
    "image_path = \"./image_face_detection/0000eda1171fe14e.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for filename in os.listdir(image_folder):\n",
    "#    image_path = os.path.join(image_folder, filename)\n",
    "img = Image.open(image_path)\n",
    "\n",
    "W, H = img.size\n",
    "\n",
    "pyramid = image_pyramid(img, scale=PYR_SCALE, minSize=ROI_SIZE)\n",
    "\n",
    "for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "    scale = W / float(image.shape[1])\n",
    "    # for each layer of the image pyramid, loop over the sliding\n",
    "    # window locations\n",
    "    for (x, y, roiOrig) in sliding_window(image, WINDOW_STEP, ROI_SIZE):\n",
    "        # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "        # *original* image dimensions\n",
    "        x = int(x * scale)\n",
    "        y = int(y * scale)\n",
    "        w = int(ROI_SIZE[0] * scale)\n",
    "        h = int(ROI_SIZE[1] * scale)\n",
    "        # take the ROI and preprocess it so we can later classify\n",
    "        # the region using Keras/TensorFlow\n",
    "        #roi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "        #roi = img_to_array(roi)\n",
    "        #roi = preprocess_input(roi)\n",
    "\n",
    "        roi_tensor_gray = transform(roiOrig)\n",
    "\n",
    "        # update our list of ROIs and associated coordinates\n",
    "        rois.append(roi_tensor_gray)\n",
    "        locs.append((x, y, x + w, y + h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorDataset\n",
    "dataset = torch.utils.data.TensorDataset(*rois)\n",
    "\n",
    "# Define batch size and other DataLoader parameters\n",
    "#batch_size = 32\n",
    "\n",
    "# Create a DataLoader\n",
    "#dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = face_detection_net(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3,4,5])\n",
    "print(x.size(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(output, dim=1)\n",
    "probs_list = probs.tolist()\n",
    "\n",
    "labels = {'valid_probs': [],\n",
    "          'boxes': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(probs_list)):\n",
    "    if (probs_list[i][1] >= 0.95):\n",
    "        box = locs[i]\n",
    "\n",
    "        labels['valid_probs'].append(probs_list[i][1])\n",
    "        labels['boxes'].append(box)\n",
    "\n",
    "\n",
    "valid_box = ops.nms(torch.Tensor(labels['boxes']), torch.Tensor(labels['valid_probs']), iou_threshold=0.3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x772 at 0x1B67D140C40>\n",
      "torch.Size([1, 772, 1024])\n"
     ]
    }
   ],
   "source": [
    "image_path = \"./image_face_detection/0000eda1171fe14e.jpg\"\n",
    "img = Image.open(image)\n",
    "\n",
    "print(img)\n",
    "\n",
    "img_tensor = transform(img) \n",
    "  \n",
    "# print the converted Torch tensor \n",
    "print(img_tensor.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the region of interest\n",
    "roi = img.crop((left, top, left + width, top + height))\n",
    "\n",
    "# Resize the region to 36x36\n",
    "#roi_resized = roi.resize((36, 36))\n",
    "\n",
    "# Convert the image to grayscale\n",
    "roi_gray = roi.convert('L')\n",
    "\n",
    "rois.append(roi)\n",
    "#locs.append((x, y, x + w, y + h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in pyramid:\n",
    "\t# determine the scale factor between the *original* image\n",
    "\t# dimensions and the *current* layer of the pyramid\n",
    "\tscale = W / float(image.shape[1])\n",
    "\t# for each layer of the image pyramid, loop over the sliding\n",
    "\t# window locations\n",
    "\tfor (x, y, roiOrig) in sliding_window(image, WIN_STEP, ROI_SIZE):\n",
    "\t\t# scale the (x, y)-coordinates of the ROI with respect to the\n",
    "\t\t# *original* image dimensions\n",
    "\t\tx = int(x * scale)\n",
    "\t\ty = int(y * scale)\n",
    "\t\tw = int(ROI_SIZE[0] * scale)\n",
    "\t\th = int(ROI_SIZE[1] * scale)\n",
    "\t\t# take the ROI and preprocess it so we can later classify\n",
    "\t\t# the region using Keras/TensorFlow\n",
    "\t\troi = cv2.resize(roiOrig, INPUT_SIZE)\n",
    "\t\troi = img_to_array(roi)\n",
    "\t\troi = preprocess_input(roi)\n",
    "\t\t# update our list of ROIs and associated coordinates\n",
    "\t\trois.append(roi)\n",
    "\t\tlocs.append((x, y, x + w, y + h))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
