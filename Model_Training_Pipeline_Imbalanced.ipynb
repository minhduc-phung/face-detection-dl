{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Imbalanced training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will try to implement with imbalanced data as the input. It refers to the situation where the distribution of classes in the target variable is not equal. In our case, we will try to train the model on a dataset that comprised much more non-face images than face images.\n",
    "Imbalanced data can cause classification algorithms to have a biased decision boundary. As such the algorithms may favor the majority class, leading to poor performance and low prediction accuracy for the minority class. We will see if the model cannot predict well the images showing a face - the minority class in our situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the necessary libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "\n",
    "from net import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use CUDA if possible\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data from the same folders and with the same transformation to Pytorch tensor as in previous parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './train_images_imbalanced'    # folder containing training images\n",
    "test_dir = './test_images'    # folder containing test images\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Grayscale(),   # transforms to gray-scale (1 input channel)\n",
    "     transforms.ToTensor(),    # transforms to Torch tensor (needed for PyTorch)\n",
    "     transforms.Normalize(mean=(0.5,),std=(0.5,))]) # subtracts mean (0.5) and devides by standard deviation (0.5) -> resulting values in (-1, +1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two pytorch datasets (train/test) \n",
    "train_data = torchvision.datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_data = torchvision.datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "valid_size = 0.2   # proportion of validation set (80% train, 20% validation)\n",
    "batch_size = 32    \n",
    "\n",
    "# Define randomly the indices of examples to use for training and for validation\n",
    "num_train = len(train_data)\n",
    "indices_train = list(range(num_train))\n",
    "np.random.shuffle(indices_train)\n",
    "split_tv = int(np.floor(valid_size * num_train))\n",
    "train_new_idx, valid_idx = indices_train[split_tv:],indices_train[:split_tv]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, instead of using the SubsetRandomSampler class, we use the ImbalancedDatasetSampler class, which will take more non-face images than face images to our training data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two \"samplers\" that will pick examples from the training and validation set in an imbalanced way\n",
    "train_sampler = SubsetRandomSampler(train_new_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# Dataloaders (take care of loading the data from disk, batch by batch, during training)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=4)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "classes = ('noface','face')  # indicates that \"1\" means \"face\" and \"0\" non-face (only used for display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see below, the percentage of face images is only around 17%, which is small compare to the number of non-face images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train samples:  25904\n",
      "number of face images is:  4349\n"
     ]
    }
   ],
   "source": [
    "print(\"number of train samples: \", len(train_sampler))\n",
    "\n",
    "nb_faces = 0\n",
    "for data, target in train_loader:\n",
    "    nb_faces += target.sum().item()\n",
    "\n",
    "print(\"number of face images is: \", nb_faces)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep the same optimizer and criterion as previous parts. Then, we train our model with the previous imbalanced training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net = net.to(device)\n",
    "n_epochs = 8\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, running_loss: 99.1066437\n",
      "epoch: 2, running_loss: 17.0050240\n",
      "epoch: 3, running_loss: 9.7368145\n",
      "epoch: 4, running_loss: 6.8766294\n",
      "epoch: 5, running_loss: 5.3132062\n",
      "epoch: 6, running_loss: 3.8671057\n",
      "epoch: 7, running_loss: 3.7035697\n",
      "epoch: 8, running_loss: 2.6331968\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "running_loss =0\n",
    "# loop over epochs: one epoch = one pass through the whole training dataset\n",
    "for epoch in range(1, n_epochs+1):  \n",
    "#   loop over iterations: one iteration = 1 batch of examples\n",
    "    running_loss =0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad() # zero the gradient buffers\n",
    "        output = net(data)\n",
    "        loss = criterion(output, target)\n",
    "        running_loss +=loss\n",
    "        loss.backward()\n",
    "        optimizer.step() # Does the update\n",
    "    print ('epoch: %d, running_loss: %5.7f' % (epoch,running_loss))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to create a classification map that contains the number of true positive, false positive, true negative, false negative of the predictions of our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 91.858941 %\n"
     ]
    }
   ],
   "source": [
    "classification_map = {\"TP\" : 0,\n",
    "                      \"FP\" : 0,\n",
    "                      \"TN\" : 0,\n",
    "                      \"FN\" : 0}\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "count = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        count+=1\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        for i in range(0,len(labels)):\n",
    "            if predicted[i].item() == labels[i].item():\n",
    "                if predicted[i].item() == 1:\n",
    "                    classification_map[\"TP\"] +=1\n",
    "                else:\n",
    "                    classification_map[\"TN\"] +=1\n",
    "            elif predicted[i].item() == 1:\n",
    "                classification_map[\"FP\"] +=1\n",
    "            else: \n",
    "                classification_map[\"FN\"] +=1\n",
    "\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %5.6f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the performance metrics of the model. The scores are good enough, except for the Recall score and F score. The Recall score calculates the ratio of good predictions of face images over total number of face images. This score is very low (only 0.23 in this case) because the model predicted many false negatives. It is coherent with the known issue with imbalanced training dataset there will be low prediction accuracy for the minority class (in our case is the face image). Since the Recall score is low, then the F score is also low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  183\n",
      "TN:  6824\n",
      "FP:  7\n",
      "FN:  614\n",
      "\n",
      "\n",
      "Specificity :  0.998975259844825\n",
      "Recall :  0.22961104140526975\n",
      "Precision :  0.9631578947368421\n",
      "Accuracy :  0.9185894074462506\n",
      "F-score :  0.3708206686930091\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (\"TP: \", classification_map[\"TP\"])\n",
    "print (\"TN: \", classification_map[\"TN\"])\n",
    "print (\"FP: \", classification_map[\"FP\"])\n",
    "print (\"FN: \", classification_map[\"FN\"])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "stats_map = {\n",
    "            \"Specificity\" : float(classification_map[\"TN\"]) / float(classification_map[\"TN\"] + classification_map[\"FP\"]),\n",
    "            \"Recall\" : float(classification_map[\"TP\"]) / float(classification_map[\"TP\"] + classification_map[\"FN\"]),\n",
    "            \"Precision\" : float(classification_map[\"TP\"]) / float(classification_map[\"TP\"] + classification_map[\"FP\"]),\n",
    "            \"Accuracy\" : float(classification_map[\"TP\"] + classification_map[\"TN\"]) / float(classification_map[\"TP\"] + classification_map[\"TN\"] + classification_map[\"FP\"] + classification_map[\"FN\"])\n",
    "        }\n",
    "stats_map[\"F-score\"] = 2.0 / float((1.0 / float(stats_map[\"Precision\"])) + (1.0 / float(stats_map[\"Recall\"])))\n",
    "\n",
    "for key, value in stats_map.items():\n",
    "    print(key, \": \", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the issue of the having imbalanced training dataset, it is necessary to avoid this problem. There are several techniques that can be implemented to reduce the effect of imbalanced data such as oversampling (adding more samples to the minority class), undersampling (removing samples from the majority class), or by using class weights. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
