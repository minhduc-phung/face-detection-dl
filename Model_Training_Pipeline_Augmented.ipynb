{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from net import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Use CUDA if possible\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './train_images'    # folder containing training images\n",
    "test_dir = './test_images'    # folder containing test images\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Grayscale(),   # transforms to gray-scale (1 input channel)\n",
    "     transforms.ToTensor(),    # transforms to Torch tensor (needed for PyTorch)\n",
    "     transforms.Normalize(mean=(0.5,),std=(0.5,))]) # subtracts mean (0.5) and devides by standard deviation (0.5) -> resulting values in (-1, +1)\n",
    "\n",
    "# Define the transformations for augmentation\n",
    "transforms_augmented = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Grayscale(), \n",
    "        transforms.RandomResizedCrop(36),\n",
    "        transforms.RandomHorizontalFlip(0.25),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5,),std=(0.5,))\n",
    "    ]),\n",
    "    'test':transforms.Compose([\n",
    "        transforms.Grayscale(), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(mean=(0.5,),std=(0.5,))])        \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two pytorch datasets (train/test) \n",
    "train_data_augmented = torchvision.datasets.ImageFolder(train_dir, transform=transforms_augmented['train'])\n",
    "test_data = torchvision.datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "valid_size = 0.2   # proportion of validation set (80% train, 20% validation)\n",
    "batch_size = 128\n",
    "\n",
    "# Define randomly the indices of examples to use for training and for validation\n",
    "num_train = len(train_data_augmented)\n",
    "indices_train = list(range(num_train))\n",
    "np.random.shuffle(indices_train)\n",
    "split_tv = int(np.floor(valid_size * num_train))\n",
    "train_new_idx, valid_idx = indices_train[split_tv:],indices_train[:split_tv]\n",
    "\n",
    "# Define two \"samplers\" that will randomly pick examples from the training and validation set\n",
    "train_sampler = SubsetRandomSampler(train_new_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders (take care of loading the data from disk, batch by batch, during training)\n",
    "train_loader = torch.utils.data.DataLoader(train_data_augmented, batch_size=batch_size, sampler=train_sampler, num_workers=4)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data_augmented, batch_size=batch_size, sampler=valid_sampler, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "classes = ('noface','face')  # indicates that \"1\" means \"face\" and \"0\" non-face (only used for display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net = net.to(device)\n",
    "n_epochs = 64\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, running_loss: 242.1486816\n",
      "epoch: 2, running_loss: 154.1290741\n",
      "epoch: 3, running_loss: 127.1257324\n",
      "epoch: 4, running_loss: 108.5217056\n",
      "epoch: 5, running_loss: 101.9097595\n",
      "epoch: 6, running_loss: 92.1827774\n",
      "epoch: 7, running_loss: 85.4989014\n",
      "epoch: 8, running_loss: 80.0293961\n",
      "epoch: 9, running_loss: 76.4777451\n",
      "epoch: 10, running_loss: 71.2520905\n",
      "epoch: 11, running_loss: 69.1499329\n",
      "epoch: 12, running_loss: 66.7689972\n",
      "epoch: 13, running_loss: 63.9659653\n",
      "epoch: 14, running_loss: 61.7210503\n",
      "epoch: 15, running_loss: 59.7289085\n",
      "epoch: 16, running_loss: 58.8025665\n",
      "epoch: 17, running_loss: 56.6952438\n",
      "epoch: 18, running_loss: 55.0989494\n",
      "epoch: 19, running_loss: 54.6415672\n",
      "epoch: 20, running_loss: 53.4042816\n",
      "epoch: 21, running_loss: 52.7668610\n",
      "epoch: 22, running_loss: 50.5787888\n",
      "epoch: 23, running_loss: 50.6909447\n",
      "epoch: 24, running_loss: 49.7424240\n",
      "epoch: 25, running_loss: 50.4244995\n",
      "epoch: 26, running_loss: 47.5126953\n",
      "epoch: 27, running_loss: 46.8149490\n",
      "epoch: 28, running_loss: 47.1924019\n",
      "epoch: 29, running_loss: 46.8043556\n",
      "epoch: 30, running_loss: 47.1565437\n",
      "epoch: 31, running_loss: 46.4793205\n",
      "epoch: 32, running_loss: 45.3574944\n",
      "epoch: 33, running_loss: 44.3095169\n",
      "epoch: 34, running_loss: 44.0457802\n",
      "epoch: 35, running_loss: 44.4828339\n",
      "epoch: 36, running_loss: 44.4829826\n",
      "epoch: 37, running_loss: 42.9616966\n",
      "epoch: 38, running_loss: 43.8160324\n",
      "epoch: 39, running_loss: 41.4453850\n",
      "epoch: 40, running_loss: 42.4271660\n",
      "epoch: 41, running_loss: 41.5192146\n",
      "epoch: 42, running_loss: 41.6623764\n",
      "epoch: 43, running_loss: 41.3940697\n",
      "epoch: 44, running_loss: 40.4400330\n",
      "epoch: 45, running_loss: 41.2428131\n",
      "epoch: 46, running_loss: 40.5949593\n",
      "epoch: 47, running_loss: 39.8121452\n",
      "epoch: 48, running_loss: 40.9302292\n",
      "epoch: 49, running_loss: 39.4125595\n",
      "epoch: 50, running_loss: 38.9246826\n",
      "epoch: 51, running_loss: 37.7960548\n",
      "epoch: 52, running_loss: 38.8276825\n",
      "epoch: 53, running_loss: 38.5925179\n",
      "epoch: 54, running_loss: 36.8946686\n",
      "epoch: 55, running_loss: 38.6585541\n",
      "epoch: 56, running_loss: 38.9894829\n",
      "epoch: 57, running_loss: 40.0223885\n",
      "epoch: 58, running_loss: 37.0797348\n",
      "epoch: 59, running_loss: 37.1399422\n",
      "epoch: 60, running_loss: 38.2797661\n",
      "epoch: 61, running_loss: 37.0514526\n",
      "epoch: 62, running_loss: 37.7890129\n",
      "epoch: 63, running_loss: 37.0404205\n",
      "epoch: 64, running_loss: 36.4491615\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "running_loss =0\n",
    "# loop over epochs: one epoch = one pass through the whole training dataset\n",
    "for epoch in range(1, n_epochs+1):  \n",
    "#   loop over iterations: one iteration = 1 batch of examples\n",
    "    running_loss =0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad() # zero the gradient buffers\n",
    "        output = net(data)\n",
    "        loss = criterion(output, target)\n",
    "        running_loss +=loss\n",
    "        loss.backward()\n",
    "        optimizer.step() # Does the update\n",
    "    print ('epoch: %d, running_loss: %5.7f' % (epoch,running_loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98.374410 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %5.6f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(net.state_dict(), './saved_model.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
