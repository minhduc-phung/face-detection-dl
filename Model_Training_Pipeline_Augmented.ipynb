{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Augmented training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will try to use an augmented training dataset to train our model. Data augmentation is a technique of artificially increasing the training set by creating modified copies of a dataset using existing data. It includes making minor changes to the dataset or generating new data points. The goal of data augmentation technique is make the data rich and sufficient and thus makes the model perform better and accurately. We will the take a look at the accuracy of the model after training with augmented dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from net import Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data augmentation technique in our case is fairly simple: it consists only of randomly flipping horizontally 50% of the original training dataset. We will see what impact it will have on the prediction of the model. Now, we proceed with the same process as the first part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './train_images'    # folder containing training images\n",
    "test_dir = './test_images'    # folder containing test images\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Grayscale(),   # transforms to gray-scale (1 input channel)\n",
    "     transforms.ToTensor(),    # transforms to Torch tensor (needed for PyTorch)\n",
    "     transforms.Normalize(mean=(0.5,),std=(0.5,))]) # subtracts mean (0.5) and devides by standard deviation (0.5) -> resulting values in (-1, +1)\n",
    "\n",
    "# Define the transformations for augmentation\n",
    "transforms_augmented = transforms.Compose([\n",
    "        transforms.Grayscale(), \n",
    "        transforms.RandomResizedCrop(36),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5,),std=(0.5,))\n",
    "])\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two pytorch datasets (train/test) \n",
    "train_data_augmented = torchvision.datasets.ImageFolder(train_dir, transform=transforms_augmented)\n",
    "test_data = torchvision.datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "valid_size = 0.2   # proportion of validation set (80% train, 20% validation)\n",
    "batch_size = 32    \n",
    "\n",
    "# Define randomly the indices of examples to use for training and for validation\n",
    "num_train = len(train_data_augmented)\n",
    "indices_train = list(range(num_train))\n",
    "np.random.shuffle(indices_train)\n",
    "split_tv = int(np.floor(valid_size * num_train))\n",
    "train_new_idx, valid_idx = indices_train[split_tv:],indices_train[:split_tv]\n",
    "\n",
    "# Define two \"samplers\" that will randomly pick examples from the training and validation set\n",
    "train_sampler = SubsetRandomSampler(train_new_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders (take care of loading the data from disk, batch by batch, during training)\n",
    "train_loader = torch.utils.data.DataLoader(train_data_augmented, batch_size=batch_size, sampler=train_sampler, num_workers=4)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data_augmented, batch_size=batch_size, sampler=valid_sampler, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "classes = ('noface','face')  # indicates that \"1\" means \"face\" and \"0\" non-face (only used for display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "n_epochs = 16\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, running_loss: 745.0629272\n",
      "epoch: 2, running_loss: 432.7305298\n",
      "epoch: 3, running_loss: 341.3478088\n",
      "epoch: 4, running_loss: 292.0621338\n",
      "epoch: 5, running_loss: 267.2956848\n",
      "epoch: 6, running_loss: 251.9956818\n",
      "epoch: 7, running_loss: 238.5132904\n",
      "epoch: 8, running_loss: 232.7164001\n",
      "epoch: 9, running_loss: 224.1664276\n",
      "epoch: 10, running_loss: 222.7513428\n",
      "epoch: 11, running_loss: 206.9162445\n",
      "epoch: 12, running_loss: 213.8405457\n",
      "epoch: 13, running_loss: 203.5930023\n",
      "epoch: 14, running_loss: 203.8099213\n",
      "epoch: 15, running_loss: 201.2661285\n",
      "epoch: 16, running_loss: 196.9024353\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "running_loss =0\n",
    "# loop over epochs: one epoch = one pass through the whole training dataset\n",
    "for epoch in range(1, n_epochs+1):  \n",
    "#   loop over iterations: one iteration = 1 batch of examples\n",
    "    running_loss =0\n",
    "    for data, target in train_loader: \n",
    "        optimizer.zero_grad() # zero the gradient buffers\n",
    "        output = net(data)\n",
    "        loss = criterion(output, target)\n",
    "        running_loss +=loss\n",
    "        loss.backward()\n",
    "        optimizer.step() # Does the update\n",
    "    print ('epoch: %d, running_loss: %5.7f' % (epoch,running_loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on test images: 97.981122 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on test images: %5.6f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the accuracy of the prediction on the test images is fairly higher than the model trained on the original training dataset. It shows that data augmentation technique can lead to better performance and higher accuracy, even with a simple technique in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "torch.save(net.state_dict(), './saved_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
