{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.ops as ops\n",
    "from net import Net\n",
    "import cv2\n",
    "import time\n",
    "import imutils\n",
    "\n",
    "import warnings\n",
    "#suppress warning\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_detection_net = Net()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "face_detection_net.load_state_dict(torch.load(\"./saved_model.pth\",map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "MAX_WIDTH = 1500\n",
    "def resize(image):\n",
    "    if image.shape[1] > MAX_WIDTH:\n",
    "        # resize the image to a fixed width, while ensuring the aspect\n",
    "        # ratio is maintained\n",
    "        resized = imutils.resize(image, width=MAX_WIDTH)\n",
    "        return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./face_detection_images/detection_test2.jpg\"\n",
    "original_image = cv2.imread(image_path)\n",
    "original_image = resize(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize OpenCV's selective search implementation and set the\n",
    "# input image\n",
    "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "ss.setBaseImage(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using *quality* selective search\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] using *quality* selective search\")\n",
    "#ss.switchToSelectiveSearchFast()\n",
    "ss.switchToSelectiveSearchQuality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] selective search took 93.5325 seconds\n",
      "35293\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "rects = ss.process()\n",
    "end = time.time()\n",
    "print(\"[INFO] selective search took {:.4f} seconds\".format(end - start))\n",
    "print(len(rects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ \n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),    \n",
    "    transforms.Resize((36,36))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "(H, W) = original_image.shape[:2]\n",
    "\n",
    "rois = []\n",
    "locs = []\n",
    "\n",
    "for (x, y, w, h) in rects:\n",
    "\t# if the width or height of the region is less than 5% of the\n",
    "\t# image width or height, ignore it (i.e., filter out small\n",
    "\t# objects that are likely false-positives)\n",
    "\tif w / float(W) < 0.05 or h / float(H) < 0.05:\n",
    "\t\tcontinue\n",
    "\t# extract the region from the input image, convert it from BGR to\n",
    "\t# RGB channel ordering\n",
    "\troi = original_image[y:y + h, x:x + w]\n",
    "\troi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\t# further preprocess by the ROI\n",
    "\n",
    "\troi_tensor_gray = transform(roi)\n",
    "\t\n",
    "\t# update our list of ROIs and associated coordinates\n",
    "\trois.append(roi_tensor_gray)\n",
    "\tlocs.append((x, y, x + w, y + h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUGGING\n",
    "# print(len(rois))\n",
    "# print(len(locs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The list of region proposals is \"stacked\" into a tensor in order to be fed into the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14173, 1, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "stacked_tensor = torch.stack(rois, dim=0)\n",
    "\n",
    "print(stacked_tensor.size())\n",
    "\n",
    "output = face_detection_net(stacked_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(output, dim=1)\n",
    "probs_list = probs.tolist()\n",
    "\n",
    "labels = {'valid_probs': [],\n",
    "          'boxes': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(probs_list)):\n",
    "    if (probs_list[i][1] >= 0.95):\n",
    "        box = locs[i]\n",
    "\n",
    "        labels['valid_probs'].append(probs_list[i][1])\n",
    "        labels['boxes'].append(box)\n",
    "\n",
    "tensor_boxes = torch.Tensor(labels['boxes'])\n",
    "tensor_probs = torch.Tensor(labels['valid_probs'])\n",
    "\n",
    "valid_box = ops.nms(tensor_boxes, tensor_probs, iou_threshold=0.1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(505, 570, 668, 691), (814, 419, 1007, 634), (827, 420, 1037, 640), (825, 420, 956, 581), (875, 416, 1064, 699), (812, 420, 1017, 679), (109, 434, 224, 644), (220, 421, 321, 533), (847, 425, 1051, 532), (847, 413, 1051, 532), (202, 411, 321, 532), (745, 310, 1046, 604), (812, 405, 1047, 634), (59, 499, 209, 610), (847, 424, 1051, 532), (218, 411, 323, 536), (812, 245, 1082, 639), (56, 494, 209, 610), (816, 420, 1051, 572), (58, 495, 209, 605), (815, 413, 1051, 639), (816, 301, 1082, 639), (825, 416, 1064, 699), (862, 522, 1008, 639), (504, 570, 668, 691), (756, 405, 1047, 634), (220, 421, 323, 533), (503, 570, 667, 687), (697, 1171, 921, 1239), (814, 419, 1040, 634), (1241, 0, 1413, 799), (473, 522, 596, 587), (816, 420, 1036, 639), (202, 408, 327, 511), (475, 522, 594, 595), (214, 411, 323, 536), (736, 1088, 1008, 1189), (594, 1059, 1074, 1280), (814, 419, 1007, 639), (594, 1059, 1079, 1220), (783, 1080, 1011, 1189), (597, 984, 1032, 1275), (501, 565, 667, 681), (597, 984, 1032, 1269), (109, 460, 215, 649), (832, 522, 1008, 639), (1266, 0, 1437, 691), (597, 984, 1032, 1271), (812, 405, 1047, 639), (816, 420, 1051, 634), (595, 1065, 1008, 1271), (812, 244, 1116, 639), (812, 244, 1056, 634), (762, 395, 1037, 640), (831, 525, 1007, 635), (1241, 0, 1413, 796), (812, 285, 1064, 699), (825, 420, 1019, 581), (58, 492, 214, 625), (812, 248, 1059, 693), (814, 419, 1040, 639), (812, 248, 1064, 699), (847, 413, 1051, 639), (214, 411, 323, 512), (220, 408, 322, 512), (1220, 0, 1400, 743), (214, 411, 323, 533), (812, 248, 1116, 699), (816, 420, 1059, 676), (816, 301, 1059, 676), (812, 420, 1020, 679), (597, 978, 1032, 1275), (592, 1014, 1053, 1270), (813, 238, 1057, 640), (594, 983, 1074, 1280), (491, 528, 667, 660), (812, 244, 1056, 639), (812, 248, 1059, 713), (813, 239, 1058, 634), (813, 239, 1058, 639)]\n",
      "tensor([20, 58, 27, 41, 10, 31, 55])\n"
     ]
    }
   ],
   "source": [
    "print(labels['boxes'])\n",
    "\n",
    "print(valid_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Color in BGR\n",
    "color = (0, 255, 0)\n",
    "thickness = 2\n",
    "\n",
    "img_color = cv2.imread(image_path)\n",
    "img_color = resize(img_color)\n",
    "  \n",
    "# Using cv2.rectangle() method \n",
    "# Draw a rectangle with blue line borders of thickness of 1 px \n",
    "for index in valid_box:\n",
    "    box = labels['boxes'][index]\n",
    "    (x,y,z,t) = box\n",
    "    cv2.rectangle(img_color, (x,y), (z,t), color, thickness)\n",
    "\n",
    "\n",
    "cv2.imshow('image', img_color)\n",
    "\n",
    "# add wait key. window waits until user presses a key\n",
    "cv2.waitKey(0)\n",
    "# and finally destroy/close all open windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUGGING\n",
    "# # loop over the region proposals in chunks (so we can better\n",
    "# # visualize them)\n",
    "# for i in range(0, len(rects), 100):\n",
    "# \t# clone the original image, so we can draw on it\n",
    "# \toutput = image.copy()\n",
    "# \t# loop over the current subset of region proposals\n",
    "# \tfor (x, y, w, h) in rects[i:i + 100]:\n",
    "# \t\t# draw the region proposal bounding box on the image\n",
    "# \t\tcolor = [random.randint(0, 255) for j in range(0, 3)]\n",
    "# \t\tcv2.rectangle(output, (x, y), (x + w, y + h), color, 2)\n",
    "# \t# show the output image\n",
    "# \tcv2.imshow(\"Output\", output)\n",
    "# \tkey = cv2.waitKey(0) & 0xFF\n",
    "# \t# if the `q` key was pressed, break from the loop\n",
    "# \tif key == ord(\"q\"):\n",
    "# \t\tbreak"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
