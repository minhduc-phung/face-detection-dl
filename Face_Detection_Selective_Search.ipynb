{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Face Detection Pipeline using Selective Search\n",
    "\n",
    "**Selective Search** is a region proposal algorithm for object detection. It over-segments the image based on intensity, color, texture, and size. Then, it groups the similar regions together using a greedy algorithm. Finally, it outputs the bounding boxes of the grouped regions as region proposals.\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "We made a face detection pipeline using the Selective Search algorithm, which is implemented in OpenCV. The pipeline is as follows:\n",
    "\n",
    "1. **Load the image** - The image is loaded in *without any preprocesing*.\n",
    "2. **Run Selective Search** - The Selective Search algorithm is run on the image to generate region proposals.\n",
    "3. **Region Filtering** - The region proposals are filtered based on their size. If the region proposal is too small, it is ignored. We used a threshold of 10% of the image size at first. However, since there was no preprocessing, for a bigger image, we got nothing out of the Selective Search algorithm. So, we reduced the threshold to 5% of the image size.\n",
    "4. **Region Proposal Preprocessing** - The region proposals are converted to grayscale and resized to 36x36.\n",
    "5. **Extract Features** - The region proposals are fed into the CNN to extract features.\n",
    "6. **Classify** - The features are classified as face or not face.\n",
    "7. **Non-Maximum Suppression** - The region proposals are filtered based on the probability of being a face. We used a threshold of 90%.\n",
    "\n",
    "Finally, the detected face regions are highlighted in the original image.\n",
    "\n",
    "## Result\n",
    "\n",
    "Even though the model has a 98% accuracy, the results were mixed. Sometimes the pipeline manages to detect most if not all the faces in an image, but for others it struggles. When it comes to speed, the pipeline took around 40 seconds for an image of size 1500x1000 even with a GPU (personally used Nvidia GeForce RTX 3050 Ti). This is because the Selective Search algorithm is very slow (or/and because there are other programs using the GPU in parallel). However, the CNN is very fast and takes only a few seconds to classify all the region proposals.\n",
    "\n",
    "![detection_test1](https://paapuruhoshi.s-ul.eu/IfZozG8a)\n",
    "![detection_test2](https://paapuruhoshi.s-ul.eu/IJkDsd9t)\n",
    "![detection_test3](https://paapuruhoshi.s-ul.eu/xglJnOku)\n",
    "![detection_test4](https://paapuruhoshi.s-ul.eu/B8WKSDX0)\n",
    "\n",
    "To explain why there are differences in the results, there are many reasons:\n",
    "- The model we used is still not good enough.\n",
    "- The Selective Search algorithm has issues due to the lack of preprocessing, which makes us having to fine-tune the hyperparameters manually for each case.\n",
    "\n",
    "## Improvements\n",
    "\n",
    "The latter can be easily fixed by resizing the image to a fixed width (for us, we only make it so that the image gets smaller if it is bigger than a certain width, small images are not resized). This way, we can use the same hyperparameters for all the images. However, the results did not improve:\n",
    "\n",
    "![detection_test2_750](https://paapuruhoshi.s-ul.eu/WfdEaCDr)\n",
    "<div style=\"text-align: center\">750x500</div>\n",
    "\n",
    "![detection_test2_1200](https://paapuruhoshi.s-ul.eu/Z1KtN7vA)\n",
    "<div style=\"text-align: center\">1200x800</div>\n",
    "\n",
    "![detection_test2_1920](https://paapuruhoshi.s-ul.eu/BIgeHTF8)\n",
    "<div style=\"text-align: center\">1920x1280</div>\n",
    "\n",
    "Therefore, we did not manage to improve the results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.ops as ops\n",
    "from net import Net\n",
    "import cv2\n",
    "import time\n",
    "import imutils\n",
    "\n",
    "import warnings\n",
    "#suppress warning\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_detection_net = Net()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "face_detection_net.load_state_dict(torch.load(\"./saved_model.pth\",map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "MAX_WIDTH = 1500\n",
    "def resize(image):\n",
    "    if image.shape[1] > MAX_WIDTH:\n",
    "        # resize the image to a fixed width, while ensuring the aspect\n",
    "        # ratio is maintained\n",
    "        resized = imutils.resize(image, width=MAX_WIDTH)\n",
    "        return resized"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./face_detection_images/detection_test2.jpg\"\n",
    "original_image = cv2.imread(image_path)\n",
    "original_image = resize(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize OpenCV's selective search implementation and set the\n",
    "# input image\n",
    "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "ss.setBaseImage(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using *quality* selective search\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] using *quality* selective search\")\n",
    "#ss.switchToSelectiveSearchFast()\n",
    "ss.switchToSelectiveSearchQuality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] selective search took 93.5325 seconds\n",
      "35293\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "rects = ss.process()\n",
    "end = time.time()\n",
    "print(\"[INFO] selective search took {:.4f} seconds\".format(end - start))\n",
    "print(len(rects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ \n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),    \n",
    "    transforms.Resize((36,36))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "(H, W) = original_image.shape[:2]\n",
    "\n",
    "rois = []\n",
    "locs = []\n",
    "\n",
    "for (x, y, w, h) in rects:\n",
    "\t# if the width or height of the region is less than 5% of the\n",
    "\t# image width or height, ignore it (i.e., filter out small\n",
    "\t# objects that are likely false-positives)\n",
    "\tif w / float(W) < 0.05 or h / float(H) < 0.05:\n",
    "\t\tcontinue\n",
    "\t# extract the region from the input image, convert it from BGR to\n",
    "\t# RGB channel ordering\n",
    "\troi = original_image[y:y + h, x:x + w]\n",
    "\troi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\t# further preprocess by the ROI\n",
    "\n",
    "\troi_tensor_gray = transform(roi)\n",
    "\t\n",
    "\t# update our list of ROIs and associated coordinates\n",
    "\trois.append(roi_tensor_gray)\n",
    "\tlocs.append((x, y, x + w, y + h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUGGING\n",
    "# print(len(rois))\n",
    "# print(len(locs))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The list of region proposals is \"stacked\" into a tensor in order to be fed into the CNN."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14173, 1, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "stacked_tensor = torch.stack(rois, dim=0)\n",
    "\n",
    "print(stacked_tensor.size())\n",
    "\n",
    "output = face_detection_net(stacked_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(output, dim=1)\n",
    "probs_list = probs.tolist()\n",
    "\n",
    "labels = {'valid_probs': [],\n",
    "          'boxes': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(probs_list)):\n",
    "    if (probs_list[i][1] >= 0.95):\n",
    "        box = locs[i]\n",
    "\n",
    "        labels['valid_probs'].append(probs_list[i][1])\n",
    "        labels['boxes'].append(box)\n",
    "\n",
    "tensor_boxes = torch.Tensor(labels['boxes'])\n",
    "tensor_probs = torch.Tensor(labels['valid_probs'])\n",
    "\n",
    "valid_box = ops.nms(tensor_boxes, tensor_probs, iou_threshold=0.1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(505, 570, 668, 691), (814, 419, 1007, 634), (827, 420, 1037, 640), (825, 420, 956, 581), (875, 416, 1064, 699), (812, 420, 1017, 679), (109, 434, 224, 644), (220, 421, 321, 533), (847, 425, 1051, 532), (847, 413, 1051, 532), (202, 411, 321, 532), (745, 310, 1046, 604), (812, 405, 1047, 634), (59, 499, 209, 610), (847, 424, 1051, 532), (218, 411, 323, 536), (812, 245, 1082, 639), (56, 494, 209, 610), (816, 420, 1051, 572), (58, 495, 209, 605), (815, 413, 1051, 639), (816, 301, 1082, 639), (825, 416, 1064, 699), (862, 522, 1008, 639), (504, 570, 668, 691), (756, 405, 1047, 634), (220, 421, 323, 533), (503, 570, 667, 687), (697, 1171, 921, 1239), (814, 419, 1040, 634), (1241, 0, 1413, 799), (473, 522, 596, 587), (816, 420, 1036, 639), (202, 408, 327, 511), (475, 522, 594, 595), (214, 411, 323, 536), (736, 1088, 1008, 1189), (594, 1059, 1074, 1280), (814, 419, 1007, 639), (594, 1059, 1079, 1220), (783, 1080, 1011, 1189), (597, 984, 1032, 1275), (501, 565, 667, 681), (597, 984, 1032, 1269), (109, 460, 215, 649), (832, 522, 1008, 639), (1266, 0, 1437, 691), (597, 984, 1032, 1271), (812, 405, 1047, 639), (816, 420, 1051, 634), (595, 1065, 1008, 1271), (812, 244, 1116, 639), (812, 244, 1056, 634), (762, 395, 1037, 640), (831, 525, 1007, 635), (1241, 0, 1413, 796), (812, 285, 1064, 699), (825, 420, 1019, 581), (58, 492, 214, 625), (812, 248, 1059, 693), (814, 419, 1040, 639), (812, 248, 1064, 699), (847, 413, 1051, 639), (214, 411, 323, 512), (220, 408, 322, 512), (1220, 0, 1400, 743), (214, 411, 323, 533), (812, 248, 1116, 699), (816, 420, 1059, 676), (816, 301, 1059, 676), (812, 420, 1020, 679), (597, 978, 1032, 1275), (592, 1014, 1053, 1270), (813, 238, 1057, 640), (594, 983, 1074, 1280), (491, 528, 667, 660), (812, 244, 1056, 639), (812, 248, 1059, 713), (813, 239, 1058, 634), (813, 239, 1058, 639)]\n",
      "tensor([20, 58, 27, 41, 10, 31, 55])\n"
     ]
    }
   ],
   "source": [
    "print(labels['boxes'])\n",
    "\n",
    "print(valid_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Color in BGR\n",
    "color = (0, 255, 0)\n",
    "thickness = 2\n",
    "\n",
    "img_color = cv2.imread(image_path)\n",
    "img_color = resize(img_color)\n",
    "  \n",
    "# Using cv2.rectangle() method \n",
    "# Draw a rectangle with blue line borders of thickness of 1 px \n",
    "for index in valid_box:\n",
    "    box = labels['boxes'][index]\n",
    "    (x,y,z,t) = box\n",
    "    cv2.rectangle(img_color, (x,y), (z,t), color, thickness)\n",
    "\n",
    "\n",
    "cv2.imshow('image', img_color)\n",
    "\n",
    "# add wait key. window waits until user presses a key\n",
    "cv2.waitKey(0)\n",
    "# and finally destroy/close all open windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEBUGGING\n",
    "# # loop over the region proposals in chunks (so we can better\n",
    "# # visualize them)\n",
    "# for i in range(0, len(rects), 100):\n",
    "# \t# clone the original image, so we can draw on it\n",
    "# \toutput = image.copy()\n",
    "# \t# loop over the current subset of region proposals\n",
    "# \tfor (x, y, w, h) in rects[i:i + 100]:\n",
    "# \t\t# draw the region proposal bounding box on the image\n",
    "# \t\tcolor = [random.randint(0, 255) for j in range(0, 3)]\n",
    "# \t\tcv2.rectangle(output, (x, y), (x + w, y + h), color, 2)\n",
    "# \t# show the output image\n",
    "# \tcv2.imshow(\"Output\", output)\n",
    "# \tkey = cv2.waitKey(0) & 0xFF\n",
    "# \t# if the `q` key was pressed, break from the loop\n",
    "# \tif key == ord(\"q\"):\n",
    "# \t\tbreak"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
