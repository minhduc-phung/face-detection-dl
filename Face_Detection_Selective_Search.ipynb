{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.ops as ops\n",
    "from net import Net\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "#suppress warning\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_detection_net = Net()\n",
    "face_detection_net.load_state_dict(torch.load(\"./saved_model.pth\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./image_face_detection/detection_test1.jpg\"\n",
    "image = cv2.imread(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize OpenCV's selective search implementation and set the\n",
    "# input image\n",
    "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "ss.setBaseImage(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] using *quality* selective search\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] using *quality* selective search\")\n",
    "#ss.switchToSelectiveSearchQuality()\n",
    "ss.switchToSelectiveSearchQuality()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16452\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "rects = ss.process()\n",
    "end = time.time()\n",
    "\n",
    "print(len(rects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ \n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),    \n",
    "    transforms.Resize((36,36))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "(H, W) = image.shape[:2]\n",
    "\n",
    "rois = []\n",
    "locs = []\n",
    "\n",
    "for (x, y, w, h) in rects:\n",
    "\t# if the width or height of the region is less than 10% of the\n",
    "\t# image width or height, ignore it (i.e., filter out small\n",
    "\t# objects that are likely false-positives)\n",
    "\tif w / float(W) < 0.1 or h / float(H) < 0.1:\n",
    "\t\tcontinue\n",
    "\t# extract the region from the input image, convert it from BGR to\n",
    "\t# RGB channel ordering\n",
    "\troi = image[y:y + h, x:x + w]\n",
    "\troi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\t# further preprocess by the ROI\n",
    "\n",
    "\troi_tensor_gray = transform(roi)\n",
    "\t\n",
    "\t# update our list of ROIs and associated coordinates\n",
    "\trois.append(roi_tensor_gray)\n",
    "\tlocs.append((x, y, x + w, y + h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4563\n",
      "4563\n"
     ]
    }
   ],
   "source": [
    "print(len(rois))\n",
    "print(len(locs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4563, 1, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "stacked_tensor = torch.stack(rois, dim=0)\n",
    "\n",
    "print(stacked_tensor.size())\n",
    "\n",
    "output = face_detection_net(stacked_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(output, dim=1)\n",
    "probs_list = probs.tolist()\n",
    "\n",
    "labels = {'valid_probs': [],\n",
    "          'boxes': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(probs_list)):\n",
    "    if (probs_list[i][1] >= 0.95):\n",
    "        box = locs[i]\n",
    "\n",
    "        labels['valid_probs'].append(probs_list[i][1])\n",
    "        labels['boxes'].append(box)\n",
    "\n",
    "tensor_boxes = torch.Tensor(labels['boxes'])\n",
    "tensor_probs = torch.Tensor(labels['valid_probs'])\n",
    "\n",
    "valid_box = ops.nms(tensor_boxes, tensor_probs, iou_threshold=0.1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(623, 124, 736, 248)\n",
      "tensor([3, 7, 6])\n"
     ]
    }
   ],
   "source": [
    "print(labels['boxes'][1])\n",
    "\n",
    "print(valid_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Blue color in BGR \n",
    "color = (255, 0, 0) \n",
    "  \n",
    "# Line thickness of 1 px \n",
    "thickness = 1\n",
    "\n",
    "img_color = cv2.imread(image_path)\n",
    "  \n",
    "# Using cv2.rectangle() method \n",
    "# Draw a rectangle with blue line borders of thickness of 1 px \n",
    "for index in valid_box:\n",
    "    box = labels['boxes'][index]\n",
    "    (x,y,z,t) = box\n",
    "    cv2.rectangle(img_color, (x,y), (z,t), color, thickness)\n",
    "\n",
    "\n",
    "cv2.imshow('image', img_color)\n",
    "\n",
    "# add wait key. window waits until user presses a key\n",
    "cv2.waitKey(0)\n",
    "# and finally destroy/close all open windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\5IF\\OT2_ML\\deep_learning_project\\Face_Detection_Selective_Search.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/5IF/OT2_ML/deep_learning_project/Face_Detection_Selective_Search.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# show the output image\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/5IF/OT2_ML/deep_learning_project/Face_Detection_Selective_Search.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m\"\u001b[39m\u001b[39mOutput\u001b[39m\u001b[39m\"\u001b[39m, output)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/5IF/OT2_ML/deep_learning_project/Face_Detection_Selective_Search.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m key \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mwaitKey(\u001b[39m0\u001b[39;49m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/5IF/OT2_ML/deep_learning_project/Face_Detection_Selective_Search.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# if the `q` key was pressed, break from the loop\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/5IF/OT2_ML/deep_learning_project/Face_Detection_Selective_Search.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39m==\u001b[39m \u001b[39mord\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mq\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loop over the region proposals in chunks (so we can better\n",
    "# visualize them)\n",
    "for i in range(0, len(rects), 100):\n",
    "\t# clone the original image so we can draw on it\n",
    "\toutput = image.copy()\n",
    "\t# loop over the current subset of region proposals\n",
    "\tfor (x, y, w, h) in rects[i:i + 100]:\n",
    "\t\t# draw the region proposal bounding box on the image\n",
    "\t\tcolor = [random.randint(0, 255) for j in range(0, 3)]\n",
    "\t\tcv2.rectangle(output, (x, y), (x + w, y + h), color, 2)\n",
    "\t# show the output image\n",
    "\tcv2.imshow(\"Output\", output)\n",
    "\tkey = cv2.waitKey(0) & 0xFF\n",
    "\t# if the `q` key was pressed, break from the loop\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
