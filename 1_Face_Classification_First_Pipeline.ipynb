{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Model Training Pipeline\n",
    "\n",
    "The code that was given contains only the hyperparameters and the dataloaders required for training the model.\n",
    "\n",
    "First of all, we coded the algorithm to train the model based on the given hyperparameters. We set the `batch_size` to be 64 and go for 32 epochs, even though it seems unnecessary as the total running loss stabilizes quickly at around 15 epochs. We used the Stochastic Gradient Descent optimizer at first with a learning rate of 0.0001. The loss function used is the Cross Entropy Loss. The model was trained on the CPU.\n",
    "\n",
    "At first, the model had a 92% accuracy which is already very good, due to the quality of the dataset that we got. However, the process was painfully slow, as it would take up to almost an hour for 20 epochs. Furthermore, the running loss was quite high, which means that the model was not learning very well.\n",
    "\n",
    "Therefore, we made many modifications to the first pipeline to improve the performance:\n",
    "- We changed to use the GPU instead of the CPU if it's available to massively speed up the training process.\n",
    "- We changed the number of workers in the dataloader to 4 to allow the data to be loaded in parallel. Adding more workers improved the performance for more powerful computers, but for some, it used too much memory and caused the training process to crash, so we settle at 4 to be sure that it works for everyone.\n",
    "- The optimizer was changed to **Adam** with a learning rate of **0.001**.\n",
    "\n",
    "These modifications cause the model to have the accuracy of around 94%, and the running loss stabilizes at a low value of 1-4. Plus, we were satisfied with our learning speed As the model hasn't reached the desired accuracy of 98% yet, we tried to improve the model in other pipelines.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from net import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Use CUDA if possible\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './train_images'    # folder containing training images\n",
    "test_dir = './test_images'    # folder containing test images\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Grayscale(),   # transforms to gray-scale (1 input channel)\n",
    "     transforms.ToTensor(),    # transforms to Torch tensor (needed for PyTorch)\n",
    "     transforms.Normalize(mean=(0.5,),std=(0.5,))]) # subtracts mean (0.5) and devides by standard deviation (0.5) -> resulting values in (-1, +1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two pytorch datasets (train/test) \n",
    "train_data = torchvision.datasets.ImageFolder(train_dir, transform=transform)\n",
    "test_data = torchvision.datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "valid_size = 0.2   # proportion of validation set (80% train, 20% validation)\n",
    "batch_size = 64\n",
    "\n",
    "# Define randomly the indices of examples to use for training and for validation\n",
    "num_train = len(train_data)\n",
    "indices_train = list(range(num_train))\n",
    "np.random.shuffle(indices_train)\n",
    "split_tv = int(np.floor(valid_size * num_train))\n",
    "train_new_idx, valid_idx = indices_train[split_tv:],indices_train[:split_tv]\n",
    "\n",
    "# Define two \"samplers\" that will randomly pick examples from the training and validation set\n",
    "train_sampler = SubsetRandomSampler(train_new_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "91720"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders (take care of loading the data from disk, batch by batch, during training)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=4)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler, num_workers=4)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "classes = ('noface','face')  # indicates that \"1\" means \"face\" and \"0\" non-face (only used for display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net = net.to(device)\n",
    "n_epochs = 32\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, running_loss: 125.2362366\n",
      "epoch: 2, running_loss: 32.6978760\n",
      "epoch: 3, running_loss: 22.5062962\n",
      "epoch: 4, running_loss: 17.6517105\n",
      "epoch: 5, running_loss: 13.6878967\n",
      "epoch: 6, running_loss: 10.4754963\n",
      "epoch: 7, running_loss: 9.5012617\n",
      "epoch: 8, running_loss: 8.0409603\n",
      "epoch: 9, running_loss: 6.7228460\n",
      "epoch: 10, running_loss: 6.2631798\n",
      "epoch: 11, running_loss: 4.6120424\n",
      "epoch: 12, running_loss: 5.2147675\n",
      "epoch: 13, running_loss: 4.7225890\n",
      "epoch: 14, running_loss: 4.1333299\n",
      "epoch: 15, running_loss: 3.9522016\n",
      "epoch: 16, running_loss: 3.6043513\n",
      "epoch: 17, running_loss: 4.1491671\n",
      "epoch: 18, running_loss: 3.0875738\n",
      "epoch: 19, running_loss: 4.0652013\n",
      "epoch: 20, running_loss: 3.0250406\n",
      "epoch: 21, running_loss: 2.5669188\n",
      "epoch: 22, running_loss: 3.5642259\n",
      "epoch: 23, running_loss: 2.5443280\n",
      "epoch: 24, running_loss: 3.4569964\n",
      "epoch: 25, running_loss: 3.1803613\n",
      "epoch: 26, running_loss: 3.4557011\n",
      "epoch: 27, running_loss: 1.7927408\n",
      "epoch: 28, running_loss: 3.5596201\n",
      "epoch: 29, running_loss: 1.9066741\n",
      "epoch: 30, running_loss: 3.3337653\n",
      "epoch: 31, running_loss: 3.2518361\n",
      "epoch: 32, running_loss: 2.0561633\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "running_loss =0\n",
    "# loop over epochs: one epoch = one pass through the whole training dataset\n",
    "for epoch in range(1, n_epochs+1):  \n",
    "#   loop over iterations: one iteration = 1 batch of examples\n",
    "    running_loss =0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad() # zero the gradient buffers\n",
    "        output = net(data)\n",
    "        loss = criterion(output, target)\n",
    "        running_loss +=loss\n",
    "        loss.backward()\n",
    "        optimizer.step() # Does the update\n",
    "    print ('epoch: %d, running_loss: %5.7f' % (epoch,running_loss))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 94.7824 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %5.4f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(net.state_dict(), './saved_model.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
