{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Face Detection using Sliding Window\n",
    "\n",
    "After the classifier was trained, the next step was to use it for face detection. We used the sliding window to create an image pyramid, which was used to extract the region of interest (ROI) from the image. The ROI was then resized to 36x36 pixels and fed into the classifier. The classifier then outputs the probability of the ROI being a face. The ROI was then classified as a face if the probability was greater than 0.95. Non-maximum suppression was then used to remove overlapping bounding boxes.\n",
    "\n",
    "## Result\n",
    "\n",
    "The bounding boxes are not perfect fit as its size is limited to a 36x36 square, but they were right on the faces. There were many false positive detections, which we were able to remove by reducing the threshold of the non-maxima suppression function, or by reducing the size of the original image. Having the NMS threshold at 5% with the image width of 1500 pixels gave us the same result as having the NMS threshold at 10% with the image width of 1200 pixels.\n",
    "\n",
    "![10%, 1500x1000](https://paapuruhoshi.s-ul.eu/H5TRW5s1)\n",
    "<div align=\"center\">10% threshold, 1500x1000 image</div>\n",
    "\n",
    "![5%, 1500x1000](https://paapuruhoshi.s-ul.eu/OQElaFCn)\n",
    "<div align=\"center\">5% threshold, 1500x1000</div>\n",
    "\n",
    "![10%, 1200x800](https://paapuruhoshi.s-ul.eu/jvi9KBm3)\n",
    "<div align=\"center\">10% threshold, 1200x800</div>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.ops as ops\n",
    "from net import Net\n",
    "import cv2\n",
    "import warnings\n",
    "from sliding_window import sliding_window\n",
    "from image_pyramid import image_pyramid\n",
    "import torch.nn.functional as F\n",
    "import imutils\n",
    "\n",
    "#suppress warning\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_detection_net = Net()\n",
    "face_detection_net.load_state_dict(torch.load(\"./saved_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "# initialize variables used for the object detection procedure\n",
    "MAX_WIDTH = 1200\n",
    "PYR_SCALE = 1.5\n",
    "WINDOW_STEP = 16\n",
    "ROI_SIZE = (128,128)\n",
    "INPUT_SIZE = (36,36)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ \n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),    \n",
    "    transforms.Resize(INPUT_SIZE)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = []\n",
    "locs = []\n",
    "\n",
    "#image_path = \"./image_face_detection/0000bee39176697a.jpg\"\n",
    "image_path = './face_detection_images/detection_test2.jpg'\n",
    "original_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "if original_image.shape[1] > MAX_WIDTH:\n",
    "\toriginal_image = imutils.resize(original_image, width=MAX_WIDTH)\n",
    "\n",
    "(H, W) = original_image.shape[:2]\n",
    "\n",
    "pyramid = image_pyramid(original_image, scale=PYR_SCALE, min_size=ROI_SIZE)\n",
    "\n",
    "for image in pyramid:\n",
    "    # determine the scale factor between the *original* image\n",
    "    # dimensions and the *current* layer of the pyramid\n",
    "    scale = W / float(image.shape[1])\n",
    "    \n",
    "    # for each layer of the image pyramid, loop over the sliding\n",
    "    # window locations\n",
    "    for (x, y, roiOrig) in sliding_window(image, WINDOW_STEP, ROI_SIZE):\n",
    "        # scale the (x, y)-coordinates of the ROI with respect to the\n",
    "        # *original* image dimensions\n",
    "        x = int(x * scale)\n",
    "        y = int(y * scale)\n",
    "        w = int(ROI_SIZE[0] * scale)\n",
    "        h = int(ROI_SIZE[1] * scale)\n",
    "        # take the ROI and preprocess it so we can later classify the region \n",
    "\n",
    "        roi_tensor_gray = transform(roiOrig)\n",
    "\n",
    "        # update our list of ROIs and associated coordinates\n",
    "        rois.append(roi_tensor_gray)\n",
    "        locs.append((x, y, x + w, y + h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4415, 1, 36, 36])\n"
     ]
    }
   ],
   "source": [
    "stacked_tensor = torch.stack(rois, dim=0)\n",
    "\n",
    "print(stacked_tensor.size())\n",
    "\n",
    "output = face_detection_net(stacked_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax(output, dim=1)\n",
    "probs_list = probs.tolist()\n",
    "\n",
    "labels = {'valid_probs': [],\n",
    "          'boxes': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 544.,  192.,  672.,  320.],\n",
      "        [ 560.,  192.,  688.,  320.],\n",
      "        [ 560.,  208.,  688.,  336.],\n",
      "        [ 496.,  224.,  624.,  352.],\n",
      "        [ 512.,  224.,  640.,  352.],\n",
      "        [ 528.,  224.,  656.,  352.],\n",
      "        [ 544.,  224.,  672.,  352.],\n",
      "        [ 560.,  224.,  688.,  352.],\n",
      "        [ 496.,  240.,  624.,  368.],\n",
      "        [ 512.,  240.,  640.,  368.],\n",
      "        [ 528.,  240.,  656.,  368.],\n",
      "        [ 544.,  240.,  672.,  368.],\n",
      "        [ 560.,  240.,  688.,  368.],\n",
      "        [  32.,  256.,  160.,  384.],\n",
      "        [ 496.,  256.,  624.,  384.],\n",
      "        [ 512.,  256.,  640.,  384.],\n",
      "        [ 528.,  256.,  656.,  384.],\n",
      "        [ 544.,  256.,  672.,  384.],\n",
      "        [   0.,  272.,  128.,  400.],\n",
      "        [  16.,  272.,  144.,  400.],\n",
      "        [  32.,  272.,  160.,  400.],\n",
      "        [  48.,  272.,  176.,  400.],\n",
      "        [ 528.,  272.,  656.,  400.],\n",
      "        [ 544.,  272.,  672.,  400.],\n",
      "        [ 768.,  272.,  896.,  400.],\n",
      "        [ 784.,  272.,  912.,  400.],\n",
      "        [ 800.,  272.,  928.,  400.],\n",
      "        [1056.,  272., 1184.,  400.],\n",
      "        [   0.,  288.,  128.,  416.],\n",
      "        [  16.,  288.,  144.,  416.],\n",
      "        [  32.,  288.,  160.,  416.],\n",
      "        [1040.,  288., 1168.,  416.],\n",
      "        [1056.,  288., 1184.,  416.],\n",
      "        [   0.,  304.,  128.,  432.],\n",
      "        [  16.,  304.,  144.,  432.],\n",
      "        [ 304.,  304.,  432.,  432.],\n",
      "        [ 512.,  304.,  640.,  432.],\n",
      "        [1024.,  304., 1152.,  432.],\n",
      "        [1040.,  304., 1168.,  432.],\n",
      "        [1056.,  304., 1184.,  432.],\n",
      "        [ 272.,  320.,  400.,  448.],\n",
      "        [ 992.,  320., 1120.,  448.],\n",
      "        [1008.,  320., 1136.,  448.],\n",
      "        [1024.,  320., 1152.,  448.],\n",
      "        [1040.,  320., 1168.,  448.],\n",
      "        [ 960.,  120., 1152.,  312.],\n",
      "        [ 504.,  168.,  696.,  360.],\n",
      "        [ 528.,  168.,  720.,  360.],\n",
      "        [ 480.,  192.,  672.,  384.],\n",
      "        [ 480.,  216.,  672.,  408.],\n",
      "        [ 504.,  216.,  696.,  408.],\n",
      "        [ 480.,  240.,  672.,  432.],\n",
      "        [ 504.,  240.,  696.,  432.]])\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(probs_list)):\n",
    "    if probs_list[i][1] >= 0.95:\n",
    "        box = locs[i]\n",
    "\n",
    "        labels['valid_probs'].append(probs_list[i][1])\n",
    "        labels['boxes'].append(box)\n",
    "\n",
    "tensor_boxes = torch.Tensor(labels['boxes'])\n",
    "print(tensor_boxes)\n",
    "tensor_probs = torch.Tensor(labels['valid_probs'])\n",
    "\n",
    "valid_box = ops.nms(tensor_boxes, tensor_probs, iou_threshold=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560, 192, 688, 320)\n",
      "tensor([ 8, 20, 42, 25, 40, 45])\n"
     ]
    }
   ],
   "source": [
    "print(labels['boxes'][1])\n",
    "\n",
    "print(valid_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[100], line 22\u001B[0m\n\u001B[0;32m     19\u001B[0m cv2\u001B[38;5;241m.\u001B[39mimshow(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m, img_color)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# add wait key. window waits until user presses a key\u001B[39;00m\n\u001B[1;32m---> 22\u001B[0m \u001B[43mcv2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwaitKey\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# and finally destroy/close all open windows\u001B[39;00m\n\u001B[0;32m     24\u001B[0m cv2\u001B[38;5;241m.\u001B[39mdestroyAllWindows()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    " \n",
    "# Blue color in BGR \n",
    "color = (255, 0, 0) \n",
    "  \n",
    "# Line thickness of 2 px\n",
    "thickness = 2\n",
    "\n",
    "img_color = cv2.imread(image_path)\n",
    "\n",
    "if img_color.shape[1] > MAX_WIDTH:\n",
    "\timg_color = imutils.resize(img_color, width=MAX_WIDTH)\n",
    "  \n",
    "# Using cv2.rectangle() method \n",
    "# Draw a rectangle with blue line borders of thickness of 1 px \n",
    "for index in valid_box:\n",
    "    box = labels['boxes'][index]\n",
    "    (x,y,z,t) = box\n",
    "    cv2.rectangle(img_color, (x,y), (z,t), color, thickness)\n",
    "\n",
    "cv2.imshow('image', img_color)\n",
    "\n",
    "# add wait key. window waits until user presses a key\n",
    "cv2.waitKey(0)\n",
    "# and finally destroy/close all open windows\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
